{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "# tf.enable_eager_execution()\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train data : ', 40000, 40000, 9, 0)\n",
      "(['../data/cifar10/23681_8.jpg', '../data/cifar10/49252_4.jpg', '../data/cifar10/38347_1.jpg'], [8, 4, 1])\n",
      "\n",
      "\n",
      "('Validation data : ', 10000, 10000, 9, 0)\n",
      "(['../data/cifar10/12385_9.jpg', '../data/cifar10/20257_0.jpg', '../data/cifar10/37042_8.jpg'], [9, 0, 8])\n",
      "\n",
      "\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe708c1cfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "src_dir = '../data/cifar10/'\n",
    "img_paths = glob(src_dir + '*.jpg')\n",
    "N = len(img_paths)\n",
    "\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "num_train = int(0.8*N)\n",
    "train_paths = [img_paths[idx[i]] for i in range(num_train)]\n",
    "val_paths = [img_paths[idx[num_train + i]] for i in range(N - num_train)]\n",
    "\n",
    "train_labels = []\n",
    "for path in train_paths:\n",
    "    name = os.path.basename(path)\n",
    "    train_labels.append(int(name[name.find('_')+1:-4]))\n",
    "print('Train data : ', len(train_paths), len(train_labels), max(train_labels), min(train_labels))\n",
    "print(train_paths[:3], train_labels[:3])\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "val_labels = []\n",
    "for path in val_paths:\n",
    "    name = os.path.basename(path)\n",
    "    val_labels.append(int(name[name.find('_')+1:-4]))\n",
    "print('Validation data : ', len(val_paths), len(val_labels), max(val_labels), min(val_labels))\n",
    "print(val_paths[:3], val_labels[:3])\n",
    "print('\\n')\n",
    "\n",
    "# Display an image\n",
    "img = cv2.imread(val_paths[1])\n",
    "print(img.shape)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "def conv2d_block(x, filters, kernel_size = (3,3), \\\n",
    "        padding='same', activation = 'relu', \\\n",
    "        batchnorm=True, pool = True, dropout = 0.25):\n",
    "    x = layers.Conv2D(filters=filters, kernel_size=kernel_size, \\\n",
    "            padding=padding, activation=activation)(x)\n",
    "    x = layers.BatchNormalization()(x) if batchnorm else x\n",
    "    x = layers.MaxPool2D(2, 2)(x) if pool else x\n",
    "    x = layers.Dropout(rate = dropout)(x)\n",
    "    return x\n",
    "\n",
    "def fc_block(x, units, activation = 'relu', dropout=0.25):\n",
    "    x = layers.Dense(units, activation)(x)\n",
    "    x = layers.Dropout(rate = dropout)(x)\n",
    "    return x\n",
    "\n",
    "def smallCNN(x, num_classes=10):\n",
    "    x = conv2d_block(x, 32, pool=False, dropout=0)\n",
    "    x = conv2d_block(x, 32, pool=True, dropout=0.25)\n",
    "    x = conv2d_block(x, 64, pool=False, dropout=0)\n",
    "    x = conv2d_block(x, 64, pool=True, dropout=0.25)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = fc_block(x, 512, dropout=0.5)\n",
    "    x = fc_block(x, num_classes, 'softmax')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe65eeb1f50>, '_model_dir': 'est_model2', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 2, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 1, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "def get_loss(name):\n",
    "    if name == 'categorical_crossentropy':\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = y, logits = y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "x = tf.keras.Input(shape=(32, 32, 3))\n",
    "y_pred = smallCNN(x)\n",
    "loss = tf.keras.losses.categorical_crossentropy\n",
    "# loss = lambda y, y_pred: get_loss('categorical_crossentropy', y, y_pred)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "model = tf.keras.Model(inputs=x, outputs=y_pred)\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "estimator = tf.keras.estimator.model_to_estimator(model, model_dir = 'est_model2', \\\n",
    "                                                config=tf.estimator.RunConfig(save_summary_steps=2,\\\n",
    "                                                                             log_step_count_steps=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datagen for estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess_data(path, label):\n",
    "    img = tf.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels = 3)\n",
    "    img = tf.image.resize_images(img, [32, 32])\n",
    "    img = img/255.0\n",
    "    label = tf.one_hot(label, 10)\n",
    "    return img, label\n",
    "\n",
    "# Augment data (translate and rotate)\n",
    "def augment_data(img, label):\n",
    "    img_tx = tf.contrib.image.rotate(img, \\\n",
    "            angles = tf.random_uniform(shape=[], minval=-10, maxval=10))\n",
    "    img_tx = tf.contrib.image.translate(img_tx, \\\n",
    "            translations = [tf.random_uniform(shape=[], minval=-10, maxval=10), \\\n",
    "                           tf.random_uniform(shape=[], minval=-10, maxval=10)])\n",
    "    return img_tx, label\n",
    "\n",
    "def input_fn(img_paths, labels, epochs, batch_size, shuffle=False, augment=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_paths, labels))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(img_paths))\n",
    "    dataset = dataset.map(preprocess_data)\n",
    "    if augment:\n",
    "        dataset = dataset.map(augment_data)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size)\n",
    "    dataset = dataset.prefetch(None)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SessionRun hooks: Time History Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class TimeHistory(tf.train.SessionRunHook):\n",
    "    def begin(self):\n",
    "        self.times = []\n",
    "\n",
    "    def before_run(self, run_context):\n",
    "        self.iter_time_start = time.time()\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        self.times.append(time.time() - self.iter_time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='est_model2/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: ('est_model2/keras/keras_model.ckpt',)\n",
      "INFO:tensorflow:Warm-starting variable: batch_normalization_v1_5/beta; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: batch_normalization_v1_4/gamma; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: batch_normalization_v1_7/beta; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_2/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: batch_normalization_v1_4/beta; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: batch_normalization_v1_5/gamma; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: conv2d_4/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_3/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: conv2d_6/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: conv2d_7/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: batch_normalization_v1_7/gamma; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: batch_normalization_v1_6/gamma; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: conv2d_6/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: conv2d_5/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_3/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: batch_normalization_v1_6/beta; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: conv2d_7/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: conv2d_4/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: conv2d_5/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into est_model2/model.ckpt.\n",
      "INFO:tensorflow:loss = 7.62295, step = 1\n",
      "INFO:tensorflow:global_step/sec: 3.46919\n",
      "INFO:tensorflow:loss = 7.39897, step = 2 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.2588\n",
      "INFO:tensorflow:loss = 8.66363, step = 3 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.53569\n",
      "INFO:tensorflow:loss = 7.98829, step = 4 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35179\n",
      "INFO:tensorflow:loss = 7.22222, step = 5 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.77394\n",
      "INFO:tensorflow:loss = 6.78727, step = 6 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.11868\n",
      "INFO:tensorflow:loss = 9.3577, step = 7 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.68716\n",
      "INFO:tensorflow:loss = 8.09786, step = 8 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95493\n",
      "INFO:tensorflow:loss = 8.00374, step = 9 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.27927\n",
      "INFO:tensorflow:loss = 8.62175, step = 10 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.55703\n",
      "INFO:tensorflow:loss = 7.05145, step = 11 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.36039\n",
      "INFO:tensorflow:loss = 8.80583, step = 12 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.13313\n",
      "INFO:tensorflow:loss = 6.92063, step = 13 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.62304\n",
      "INFO:tensorflow:loss = 9.32975, step = 14 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.06829\n",
      "INFO:tensorflow:loss = 6.73571, step = 15 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.07763\n",
      "INFO:tensorflow:loss = 7.67968, step = 16 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.83681\n",
      "INFO:tensorflow:loss = 6.65423, step = 17 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.01514\n",
      "INFO:tensorflow:loss = 7.62127, step = 18 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.79445\n",
      "INFO:tensorflow:loss = 7.19046, step = 19 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.48565\n",
      "INFO:tensorflow:loss = 7.03817, step = 20 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.15246\n",
      "INFO:tensorflow:loss = 8.7819, step = 21 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.53256\n",
      "INFO:tensorflow:loss = 8.04716, step = 22 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.23497\n",
      "INFO:tensorflow:loss = 7.80084, step = 23 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.59764\n",
      "INFO:tensorflow:loss = 6.76994, step = 24 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.85937\n",
      "INFO:tensorflow:loss = 8.1165, step = 25 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.5521\n",
      "INFO:tensorflow:loss = 5.85296, step = 26 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.3231\n",
      "INFO:tensorflow:loss = 7.77965, step = 27 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.3603\n",
      "INFO:tensorflow:loss = 5.26697, step = 28 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.62738\n",
      "INFO:tensorflow:loss = 6.04894, step = 29 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35898\n",
      "INFO:tensorflow:loss = 7.32705, step = 30 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.6184\n",
      "INFO:tensorflow:loss = 7.86446, step = 31 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.10169\n",
      "INFO:tensorflow:loss = 8.48933, step = 32 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.81588\n",
      "INFO:tensorflow:loss = 8.72662, step = 33 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.54725\n",
      "INFO:tensorflow:loss = 6.90746, step = 34 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.17188\n",
      "INFO:tensorflow:loss = 7.37806, step = 35 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.31984\n",
      "INFO:tensorflow:loss = 5.14018, step = 36 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.26829\n",
      "INFO:tensorflow:loss = 7.53351, step = 37 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.55738\n",
      "INFO:tensorflow:loss = 6.0066, step = 38 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.55274\n",
      "INFO:tensorflow:loss = 7.1421, step = 39 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.54288\n",
      "INFO:tensorflow:loss = 6.47159, step = 40 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.18969\n",
      "INFO:tensorflow:loss = 7.76372, step = 41 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.17715\n",
      "INFO:tensorflow:loss = 4.80841, step = 42 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.53975\n",
      "INFO:tensorflow:loss = 5.25435, step = 43 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.44284\n",
      "INFO:tensorflow:loss = 5.38306, step = 44 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.49966\n",
      "INFO:tensorflow:loss = 6.54552, step = 45 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.5894\n",
      "INFO:tensorflow:loss = 7.26368, step = 46 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.47177\n",
      "INFO:tensorflow:loss = 6.77288, step = 47 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.56586\n",
      "INFO:tensorflow:loss = 4.49089, step = 48 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.445\n",
      "INFO:tensorflow:loss = 4.69286, step = 49 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.61162\n",
      "INFO:tensorflow:loss = 7.01181, step = 50 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.54558\n",
      "INFO:tensorflow:loss = 6.01241, step = 51 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.82345\n",
      "INFO:tensorflow:loss = 7.34184, step = 52 (0.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.64366\n",
      "INFO:tensorflow:loss = 6.105, step = 53 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.56882\n",
      "INFO:tensorflow:loss = 6.38229, step = 54 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.99124\n",
      "INFO:tensorflow:loss = 6.08437, step = 55 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.77072\n",
      "INFO:tensorflow:loss = 6.86984, step = 56 (0.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.57836\n",
      "INFO:tensorflow:loss = 5.66585, step = 57 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.19947\n",
      "INFO:tensorflow:loss = 3.8578, step = 58 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.13855\n",
      "INFO:tensorflow:loss = 6.87449, step = 59 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.71174\n",
      "INFO:tensorflow:loss = 7.32373, step = 60 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.32171\n",
      "INFO:tensorflow:loss = 5.16692, step = 61 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.4068\n",
      "INFO:tensorflow:loss = 5.25432, step = 62 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8804\n",
      "INFO:tensorflow:loss = 5.72144, step = 63 (0.092 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 63 into est_model2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.72144.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7fe65eb1e0d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "\n",
    "train_paths = train_paths[:1000]\n",
    "train_labels = train_labels[:1000]\n",
    "train_input = lambda: input_fn(train_paths, train_labels,\n",
    "                              epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, augment=True)\n",
    "hook_time = TimeHistory()\n",
    "estimator.train(input_fn = train_input, hooks = [hook_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "print(len(hook_time.times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /home/surya/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/metrics_impl.py:363: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-24T06:51:34Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /home/surya/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from est_model1/model.ckpt-63\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-24-06:52:03\n",
      "INFO:tensorflow:Saving dict for global step 63: categorical_accuracy = 0.0989, global_step = 63, loss = 4.09097\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 63: est_model1/model.ckpt-63\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'categorical_accuracy': 0.098899998, 'global_step': 63, 'loss': 4.090971}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(lambda:input_fn(val_paths,\n",
    "                                   val_labels,\n",
    "                                   epochs=1,\n",
    "                                   batch_size=64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
